{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b9e8a6-329b-4da5-944e-33e6bdc98ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd662972-cdff-4fd3-9ff3-e3b1877b0abb",
   "metadata": {},
   "source": [
    "# Initializing the inputs, weights and biases, defining the target output and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d19b8b5-6e44-431f-9cdb-c969d2ddc35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([1.0, -2.0, 3.0])\n",
    "weights = np.array([-3.0, -1.0, 2.0])\n",
    "bias = 1.0\n",
    "target_output = 0.0\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39a67a-5cba-4c46-b392-03a0bf4177c5",
   "metadata": {},
   "source": [
    "# Defining relu activation and derivative of relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bdc10ab-5cc8-4c42-8d8c-562127ad7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7472f8-6aaa-4b0e-b9e7-7d4c6fe1be92",
   "metadata": {},
   "source": [
    "# Applying Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d9a5062-ae57-4cae-a7a8-8c2f1d3a2718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 36.0\n",
      "Epoch: 1 loss: 33.872399999999985\n",
      "Epoch: 2 loss: 31.870541159999995\n",
      "Epoch: 3 loss: 29.98699217744401\n",
      "Epoch: 4 loss: 28.21476093975706\n",
      "Epoch: 5 loss: 26.54726856821742\n",
      "Epoch: 6 loss: 24.978324995835766\n",
      "Epoch: 7 loss: 23.502105988581878\n",
      "Epoch: 8 loss: 22.113131524656684\n",
      "Epoch: 9 loss: 20.80624545154949\n",
      "Epoch: 10 loss: 19.576596345362915\n",
      "Epoch: 11 loss: 18.419619501351963\n",
      "Epoch: 12 loss: 17.331019988822064\n",
      "Epoch: 13 loss: 16.306756707482677\n",
      "Epoch: 14 loss: 15.343027386070442\n",
      "Epoch: 15 loss: 14.43625446755368\n",
      "Epoch: 16 loss: 13.583071828521268\n",
      "Epoch: 17 loss: 12.780312283455652\n",
      "Epoch: 18 loss: 12.024995827503426\n",
      "Epoch: 19 loss: 11.314318574097976\n",
      "Epoch: 20 loss: 10.645642346368787\n",
      "Epoch: 21 loss: 10.016484883698395\n",
      "Epoch: 22 loss: 9.424510627071816\n",
      "Epoch: 23 loss: 8.867522049011871\n",
      "Epoch: 24 loss: 8.34345149591527\n",
      "Epoch: 25 loss: 7.850353512506679\n",
      "Epoch: 26 loss: 7.386397619917536\n",
      "Epoch: 27 loss: 6.949861520580408\n",
      "Epoch: 28 loss: 6.539124704714106\n",
      "Epoch: 29 loss: 6.152662434665503\n",
      "Epoch: 30 loss: 5.7890400847767705\n",
      "Epoch: 31 loss: 5.446907815766464\n",
      "Epoch: 32 loss: 5.124995563854671\n",
      "Epoch: 33 loss: 4.8221083260308575\n",
      "Epoch: 34 loss: 4.537121723962434\n",
      "Epoch: 35 loss: 4.268977830076255\n",
      "Epoch: 36 loss: 4.016681240318748\n",
      "Epoch: 37 loss: 3.7792953790159096\n",
      "Epoch: 38 loss: 3.5559390221160707\n",
      "Epoch: 39 loss: 3.345783025909011\n",
      "Epoch: 40 loss: 3.148047249077789\n",
      "Epoch: 41 loss: 2.9619976566572896\n",
      "Epoch: 42 loss: 2.786943595148845\n",
      "Epoch: 43 loss: 2.622235228675549\n",
      "Epoch: 44 loss: 2.4672611266608238\n",
      "Epoch: 45 loss: 2.321445994075166\n",
      "Epoch: 46 loss: 2.1842485358253256\n",
      "Epoch: 47 loss: 2.0551594473580463\n",
      "Epoch: 48 loss: 1.9336995240191863\n",
      "Epoch: 49 loss: 1.8194178821496518\n",
      "Epoch: 50 loss: 1.7118902853146067\n",
      "Epoch: 51 loss: 1.6107175694525138\n",
      "Epoch: 52 loss: 1.515524161097869\n",
      "Epoch: 53 loss: 1.4259566831769857\n",
      "Epoch: 54 loss: 1.3416826432012259\n",
      "Epoch: 55 loss: 1.2623891989880334\n",
      "Epoch: 56 loss: 1.18778199732784\n",
      "Epoch: 57 loss: 1.1175840812857634\n",
      "Epoch: 58 loss: 1.0515348620817766\n",
      "Epoch: 59 loss: 0.9893891517327431\n",
      "Epoch: 60 loss: 0.930916252865338\n",
      "Epoch: 61 loss: 0.8758991023209968\n",
      "Epoch: 62 loss: 0.8241334653738256\n",
      "Epoch: 63 loss: 0.7754271775702323\n",
      "Epoch: 64 loss: 0.7295994313758316\n",
      "Epoch: 65 loss: 0.6864801049815187\n",
      "Epoch: 66 loss: 0.6459091307771115\n",
      "Epoch: 67 loss: 0.6077359011481847\n",
      "Epoch: 68 loss: 0.571818709390327\n",
      "Epoch: 69 loss: 0.5380242236653578\n",
      "Epoch: 70 loss: 0.5062269920467349\n",
      "Epoch: 71 loss: 0.47630897681677353\n",
      "Epoch: 72 loss: 0.4481591162869011\n",
      "Epoch: 73 loss: 0.4216729125143454\n",
      "Epoch: 74 loss: 0.3967520433847474\n",
      "Epoch: 75 loss: 0.3733039976207088\n",
      "Epoch: 76 loss: 0.35124173136132436\n",
      "Epoch: 77 loss: 0.3304833450378702\n",
      "Epoch: 78 loss: 0.3109517793461322\n",
      "Epoch: 79 loss: 0.29257452918677535\n",
      "Epoch: 80 loss: 0.27528337451183676\n",
      "Epoch: 81 loss: 0.25901412707818716\n",
      "Epoch: 82 loss: 0.24370639216786655\n",
      "Epoch: 83 loss: 0.22930334439074554\n",
      "Epoch: 84 loss: 0.21575151673725296\n",
      "Epoch: 85 loss: 0.2030006020980815\n",
      "Epoch: 86 loss: 0.1910032665140846\n",
      "Epoch: 87 loss: 0.17971497346310225\n",
      "Epoch: 88 loss: 0.16909381853143338\n",
      "Epoch: 89 loss: 0.1591003738562249\n",
      "Epoch: 90 loss: 0.14969754176132236\n",
      "Epoch: 91 loss: 0.14085041704322837\n",
      "Epoch: 92 loss: 0.13252615739597357\n",
      "Epoch: 93 loss: 0.12469386149387143\n",
      "Epoch: 94 loss: 0.11732445427958357\n",
      "Epoch: 95 loss: 0.1103905790316602\n",
      "Epoch: 96 loss: 0.1038664958108892\n",
      "Epoch: 97 loss: 0.09772798590846558\n",
      "Epoch: 98 loss: 0.09195226194127534\n",
      "Epoch: 99 loss: 0.08651788326054576\n",
      "Epoch: 100 loss: 0.08140467635984766\n",
      "Epoch: 101 loss: 0.07659365998698062\n",
      "Epoch: 102 loss: 0.07206697468175022\n",
      "Epoch: 103 loss: 0.06780781647805834\n",
      "Epoch: 104 loss: 0.06380037452420508\n",
      "Epoch: 105 loss: 0.06002977238982451\n",
      "Epoch: 106 loss: 0.056482012841585764\n",
      "Epoch: 107 loss: 0.05314392588264784\n",
      "Epoch: 108 loss: 0.050003119862983315\n",
      "Epoch: 109 loss: 0.04704793547908108\n",
      "Epoch: 110 loss: 0.044267402492267266\n",
      "Epoch: 111 loss: 0.04165119900497404\n",
      "Epoch: 112 loss: 0.03918961314378035\n",
      "Epoch: 113 loss: 0.036873507006982977\n",
      "Epoch: 114 loss: 0.034694282742870286\n",
      "Epoch: 115 loss: 0.032643850632766785\n",
      "Epoch: 116 loss: 0.030714599060370322\n",
      "Epoch: 117 loss: 0.028899366255902493\n",
      "Epoch: 118 loss: 0.027191413710178584\n",
      "Epoch: 119 loss: 0.025584401159906914\n",
      "Epoch: 120 loss: 0.024072363051356495\n",
      "Epoch: 121 loss: 0.02264968639502138\n",
      "Epoch: 122 loss: 0.021311089929075627\n",
      "Epoch: 123 loss: 0.02005160451426725\n",
      "Epoch: 124 loss: 0.018866554687474075\n",
      "Epoch: 125 loss: 0.017751541305444478\n",
      "Epoch: 126 loss: 0.016702425214292563\n",
      "Epoch: 127 loss: 0.015715311884128023\n",
      "Epoch: 128 loss: 0.014786536951776086\n",
      "Epoch: 129 loss: 0.013912652617925996\n",
      "Epoch: 130 loss: 0.013090414848206543\n",
      "Epoch: 131 loss: 0.012316771330677616\n",
      "Epoch: 132 loss: 0.011588850145034585\n",
      "Epoch: 133 loss: 0.010903949101463065\n",
      "Epoch: 134 loss: 0.010259525709566468\n",
      "Epoch: 135 loss: 0.00965318774013127\n",
      "Epoch: 136 loss: 0.009082684344689475\n",
      "Epoch: 137 loss: 0.008545897699918217\n",
      "Epoch: 138 loss: 0.008040835145853157\n",
      "Epoch: 139 loss: 0.007565621788733219\n",
      "Epoch: 140 loss: 0.0071184935410191314\n",
      "Epoch: 141 loss: 0.0066977905727448606\n",
      "Epoch: 142 loss: 0.0063019511498957235\n",
      "Epoch: 143 loss: 0.0059295058369368625\n",
      "Epoch: 144 loss: 0.005579072041973911\n",
      "Epoch: 145 loss: 0.005249348884293189\n",
      "Epoch: 146 loss: 0.004939112365231465\n",
      "Epoch: 147 loss: 0.004647210824446307\n",
      "Epoch: 148 loss: 0.004372560664721486\n",
      "Epoch: 149 loss: 0.004114142329436494\n",
      "Epoch: 150 loss: 0.003870996517766834\n",
      "Epoch: 151 loss: 0.0036422206235667827\n",
      "Epoch: 152 loss: 0.003426965384714017\n",
      "Epoch: 153 loss: 0.0032244317304774505\n",
      "Epoch: 154 loss: 0.0030338678152062068\n",
      "Epoch: 155 loss: 0.0028545662273275238\n",
      "Epoch: 156 loss: 0.002685861363292443\n",
      "Epoch: 157 loss: 0.002527126956721865\n",
      "Epoch: 158 loss: 0.0023777737535795864\n",
      "Epoch: 159 loss: 0.00223724732474303\n",
      "Epoch: 160 loss: 0.0021050260078507234\n",
      "Epoch: 161 loss: 0.0019806189707867374\n",
      "Epoch: 162 loss: 0.0018635643896132343\n",
      "Epoch: 163 loss: 0.0017534277341871227\n",
      "Epoch: 164 loss: 0.001649800155096641\n",
      "Epoch: 165 loss: 0.0015522969659304577\n",
      "Epoch: 166 loss: 0.001460556215243966\n",
      "Epoch: 167 loss: 0.0013742373429230384\n",
      "Epoch: 168 loss: 0.0012930199159562866\n",
      "Epoch: 169 loss: 0.0012166024389232565\n",
      "Epoch: 170 loss: 0.0011447012347829176\n",
      "Epoch: 171 loss: 0.0010770493918072417\n",
      "Epoch: 172 loss: 0.0010133957727514104\n",
      "Epoch: 173 loss: 0.0009535040825818078\n",
      "Epoch: 174 loss: 0.0008971519913012032\n",
      "Epoch: 175 loss: 0.0008441303086153036\n",
      "Epoch: 176 loss: 0.0007942422073761319\n",
      "Epoch: 177 loss: 0.0007473024929201971\n",
      "Epoch: 178 loss: 0.0007031369155886336\n",
      "Epoch: 179 loss: 0.0006615815238773228\n",
      "Epoch: 180 loss: 0.0006224820558161947\n",
      "Epoch: 181 loss: 0.0005856933663174669\n",
      "Epoch: 182 loss: 0.0005510788883681015\n",
      "Epoch: 183 loss: 0.0005185101260655451\n",
      "Epoch: 184 loss: 0.00048786617761505856\n",
      "Epoch: 185 loss: 0.00045903328651801555\n",
      "Epoch: 186 loss: 0.0004319044192847635\n",
      "Epoch: 187 loss: 0.0004063788681050637\n",
      "Epoch: 188 loss: 0.0003823618770000461\n",
      "Epoch: 189 loss: 0.0003597642900693548\n",
      "Epoch: 190 loss: 0.0003385022205262612\n",
      "Epoch: 191 loss: 0.00031849673929316324\n",
      "Epoch: 192 loss: 0.0002996735820009465\n",
      "Epoch: 193 loss: 0.0002819628733046985\n",
      "Epoch: 194 loss: 0.0002652988674923804\n",
      "Epoch: 195 loss: 0.0002496197044235683\n",
      "Epoch: 196 loss: 0.00023486717989212869\n",
      "Epoch: 197 loss: 0.00022098652956051694\n",
      "Epoch: 198 loss: 0.0002079262256634926\n",
      "Epoch: 199 loss: 0.00019563778572677352\n",
      "Final weights: [-3.3990955  -0.20180899  0.80271349]\n",
      "Final biases: 0.6009044964039992\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    # neuron output\n",
    "    neuron_output = np.dot(inputs, weights) + bias\n",
    "    relu_output = activation_relu(neuron_output)\n",
    "    \n",
    "    # square loss\n",
    "    loss = (relu_output - target_output)**2\n",
    "\n",
    "    # Backward pass (Backpropagation)\n",
    "    dloss_drelu = 2 * relu_output\n",
    "    drelu_dsum = derivative_relu(neuron_output)\n",
    "    dmul_dweights = inputs\n",
    "    dbias = 1.0\n",
    "\n",
    "    dloss_dweights = dloss_drelu * drelu_dsum * dmul_dweights\n",
    "    dloss_dbias = dloss_drelu * drelu_dsum * dbias\n",
    "\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    bias -= learning_rate * dloss_dbias\n",
    "\n",
    "    print(\"Epoch:\", epoch, \"loss:\", loss)\n",
    "\n",
    "print(\"Final weights:\", weights)\n",
    "print(\"Final biases:\", bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_from_scratch",
   "language": "python",
   "name": "nn_from_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
