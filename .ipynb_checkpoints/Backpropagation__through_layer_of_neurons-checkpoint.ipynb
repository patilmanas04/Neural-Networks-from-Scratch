{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c68d7d0-0eab-4660-9cb9-9f9a2dde2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b60678-fb73-47e3-a7a2-99b8631cf38f",
   "metadata": {},
   "source": [
    "# Defining relu function and derivative of relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "298c0c72-a4f0-4363-875a-875a5f258a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x>1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b22db8-7b27-470f-a744-435cbff222ad",
   "metadata": {},
   "source": [
    "# Backpropagating through the whole layer of neurons having 3 neurons and 4 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "819efeca-364f-4670-8bfd-6704a41d8eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 466.56000000000006\n",
      "Epoch: 2 Loss: 309.14078976\n",
      "Epoch: 3 Loss: 204.8354507318169\n",
      "Epoch: 4 Loss: 140.81821938267515\n",
      "Epoch: 5 Loss: 108.06052191699968\n",
      "Epoch: 6 Loss: 82.92305106657953\n",
      "Epoch: 7 Loss: 63.63315923526754\n",
      "Epoch: 8 Loss: 48.83055920132266\n",
      "Epoch: 9 Loss: 37.47139919767418\n",
      "Epoch: 10 Loss: 28.754652430714412\n",
      "Epoch: 11 Loss: 22.06563016367191\n",
      "Epoch: 12 Loss: 16.932635012477895\n",
      "Epoch: 13 Loss: 14.840512693931482\n",
      "Epoch: 14 Loss: 13.057336050679448\n",
      "Epoch: 15 Loss: 11.488418780174012\n",
      "Epoch: 16 Loss: 10.108016333223423\n",
      "Epoch: 17 Loss: 8.893477522688627\n",
      "Epoch: 18 Loss: 7.824872837472455\n",
      "Epoch: 19 Loss: 6.884667416813112\n",
      "Epoch: 20 Loss: 6.057433318678514\n",
      "Epoch: 21 Loss: 5.32959636083938\n",
      "Epoch: 22 Loss: 4.689213380506364\n",
      "Epoch: 23 Loss: 4.1257762575582415\n",
      "Epoch: 24 Loss: 3.630039485555074\n",
      "Epoch: 25 Loss: 3.1938684611287176\n",
      "Epoch: 26 Loss: 2.8101060023133355\n",
      "Epoch: 27 Loss: 2.472454905499374\n",
      "Epoch: 28 Loss: 2.1753746138741916\n",
      "Epoch: 29 Loss: 1.9139903017695243\n",
      "Epoch: 30 Loss: 1.6840128830701058\n",
      "Epoch: 31 Loss: 1.4816686310919343\n",
      "Epoch: 32 Loss: 1.3036372550544517\n",
      "Epoch: 33 Loss: 1.146997417036129\n",
      "Epoch: 34 Loss: 1.0091787953947358\n",
      "Epoch: 35 Loss: 0.8879199080552856\n",
      "Epoch: 36 Loss: 0.7812310035829948\n",
      "Epoch: 37 Loss: 0.6873614111164769\n",
      "Epoch: 38 Loss: 0.604770813402365\n",
      "Epoch: 39 Loss: 0.5321039715471906\n",
      "Epoch: 40 Loss: 0.4681684867419663\n",
      "Epoch: 41 Loss: 0.4119152340489983\n",
      "Epoch: 42 Loss: 0.362421147186607\n",
      "Epoch: 43 Loss: 0.31887407182525307\n",
      "Epoch: 44 Loss: 0.280559438851018\n",
      "Epoch: 45 Loss: 0.246848538916435\n",
      "Epoch: 46 Loss: 0.21718820587439186\n",
      "Epoch: 47 Loss: 0.19109173980934832\n",
      "Epoch: 48 Loss: 0.16813092072081634\n",
      "Epoch: 49 Loss: 0.147928981810686\n",
      "Epoch: 50 Loss: 0.13015442707224115\n",
      "Epoch: 51 Loss: 0.11451559173294888\n",
      "Epoch: 52 Loss: 0.10075585629268467\n",
      "Epoch: 53 Loss: 0.08864943562398082\n",
      "Epoch: 54 Loss: 0.07799767403714583\n",
      "Epoch: 55 Loss: 0.06862578551553845\n",
      "Epoch: 56 Loss: 0.06037998563113338\n",
      "Epoch: 57 Loss: 0.05312496807763898\n",
      "Epoch: 58 Loss: 0.04674168441330218\n",
      "Epoch: 59 Loss: 0.04112539058093742\n",
      "Epoch: 60 Loss: 0.03618392815029436\n",
      "Epoch: 61 Loss: 0.03183621207946751\n",
      "Epoch: 62 Loss: 0.02801090018084702\n",
      "Epoch: 63 Loss: 0.024645222458717173\n",
      "Epoch: 64 Loss: 0.021683951108967595\n",
      "Epoch: 65 Loss: 0.019078494279518486\n",
      "Epoch: 66 Loss: 0.016786098720868677\n",
      "Epoch: 67 Loss: 0.014769148242963949\n",
      "Epoch: 68 Loss: 0.01299454646668241\n",
      "Epoch: 69 Loss: 0.011433173741431704\n",
      "Epoch: 70 Loss: 0.010059409317356272\n",
      "Epoch: 71 Loss: 0.008850710931419982\n",
      "Epoch: 72 Loss: 0.0077872449087443025\n",
      "Epoch: 73 Loss: 0.006851560709489203\n",
      "Epoch: 74 Loss: 0.006028304580879835\n",
      "Epoch: 75 Loss: 0.0053039676156596185\n",
      "Epoch: 76 Loss: 0.004666664082832449\n",
      "Epoch: 77 Loss: 0.004105936393295617\n",
      "Epoch: 78 Loss: 0.0036125835000227987\n",
      "Epoch: 79 Loss: 0.0031785099169940757\n",
      "Epoch: 80 Loss: 0.0027965928794077364\n",
      "Epoch: 81 Loss: 0.0024605654653895997\n",
      "Epoch: 82 Loss: 0.002164913761330266\n",
      "Epoch: 83 Loss: 0.0019047863834238392\n",
      "Epoch: 84 Loss: 0.0016759148707371821\n",
      "Epoch: 85 Loss: 0.0014745436435288646\n",
      "Epoch: 86 Loss: 0.0012973683774970247\n",
      "Epoch: 87 Loss: 0.0011414817827304945\n",
      "Epoch: 88 Loss: 0.0010043258976447356\n",
      "Epoch: 89 Loss: 0.0008836501150873316\n",
      "Epoch: 90 Loss: 0.0007774742518588991\n",
      "Epoch: 91 Loss: 0.0006840560556525436\n",
      "Epoch: 92 Loss: 0.0006018626162295485\n",
      "Epoch: 93 Loss: 0.0005295452117138713\n",
      "Epoch: 94 Loss: 0.00046591717725518654\n",
      "Epoch: 95 Loss: 0.00040993443290490873\n",
      "Epoch: 96 Loss: 0.0003606783511847852\n",
      "Epoch: 97 Loss: 0.0003173406832198254\n",
      "Epoch: 98 Loss: 0.0002792102960868725\n",
      "Epoch: 99 Loss: 0.00024566150375025293\n",
      "Optimized set of weights ->\n",
      "[[-0.26404188 -0.52808375 -0.79212563 -1.05616751]\n",
      " [ 0.13595812 -0.12808375 -0.39212563 -0.65616751]\n",
      " [ 0.53595812  0.27191625  0.00787437 -0.25616751]]\n",
      "Optimized set of biases ->\n",
      "[-0.26404188 -0.16404188 -0.06404188]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([1, 2, 3, 4])\n",
    "weights = np.array([[.1, .2, .3, .4],\n",
    "                    [.5, .6, .7, .8],\n",
    "                    [.9, 1.0, 1.1, 1.2]])\n",
    "biases = np.array([.1, .2, .3])\n",
    "true_value = 0\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(99):\n",
    "    # forward pass and output sumission\n",
    "    z = np.dot(weights, inputs) + biases\n",
    "    a = relu(z)\n",
    "    y = np.sum(a)\n",
    "    \n",
    "    loss = y ** 2\n",
    "\n",
    "    print(f\"Epoch: {epoch+1} Loss: {loss}\")\n",
    "\n",
    "    dL_dy = 2 * y - true_value\n",
    "    dy_da = np.ones_like(a)\n",
    "\n",
    "    da_dz = derivative_relu(z)\n",
    "\n",
    "    # Gradient of loss with respect to z\n",
    "    dL_dz = dL_dy * dy_da\n",
    "\n",
    "    # Gradient of loss with respect to w and with respect to b\n",
    "    dL_dw = np.outer(dL_dz, inputs)\n",
    "    dL_db = dL_dz\n",
    "\n",
    "    weights -= learning_rate * dL_dw\n",
    "    biases -= learning_rate * dL_db\n",
    "\n",
    "print(\"Optimized set of weights ->\")\n",
    "print(weights)\n",
    "print(\"Optimized set of biases ->\")\n",
    "print(biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
