{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ccc9d137-faea-4a43-9476-a809020ff351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697fc36-3d39-47f9-ab4e-ba955cc17576",
   "metadata": {},
   "source": [
    "# Initializing inputs, weights, biases, true value and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b431c6d1-33c4-4634-952f-e82f3259ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([1.2, -0.7, 3.0])\n",
    "weights = np.array([0.8, -1.5, 2.4])\n",
    "bias = 0.5\n",
    "true_value = 5.0\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9759b2-01ba-4079-8348-6ddfab29c271",
   "metadata": {},
   "source": [
    "# Defining relu activation function and derivative of relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7522b2f7-48b5-4e2f-aef2-fc6fb5b3179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba6115-fb9c-4072-9632-8e3ce14b4fbc",
   "metadata": {},
   "source": [
    "# Apply backpropagation on single neuron using gradient decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "abb7e01d-110c-446c-9825-36ba2c87910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 22.18409999999999\n",
      "Epoch: 2 Loss: 12.860787405635993\n",
      "Epoch: 3 Loss: 7.455783768237857\n",
      "Epoch: 4 Loss: 4.322341225729185\n",
      "Epoch: 5 Loss: 2.5057907058983298\n",
      "Epoch: 6 Loss: 1.4526819456988131\n",
      "Epoch: 7 Loss: 0.8421632462726946\n",
      "Epoch: 8 Loss: 0.48822726507513997\n",
      "Epoch: 9 Loss: 0.2830399728529198\n",
      "Epoch: 10 Loss: 0.16408675214042395\n",
      "Epoch: 11 Loss: 0.09512600625489874\n",
      "Epoch: 12 Loss: 0.05514739580111172\n",
      "Epoch: 13 Loss: 0.031970597561882835\n",
      "Epoch: 14 Loss: 0.018534313245726436\n",
      "Epoch: 15 Loss: 0.010744896676572597\n",
      "Epoch: 16 Loss: 0.006229138520513436\n",
      "Epoch: 17 Loss: 0.0036112182253317036\n",
      "Epoch: 18 Loss: 0.0020935313973228214\n",
      "Epoch: 19 Loss: 0.0012136828732287783\n",
      "Epoch: 20 Loss: 0.0007036083235495789\n",
      "Epoch: 21 Loss: 0.0004079028252670542\n",
      "Epoch: 22 Loss: 0.00023647348857596\n",
      "Epoch: 23 Loss: 0.0001370907660731978\n",
      "Epoch: 24 Loss: 7.947562433198976e-05\n",
      "Epoch: 25 Loss: 4.607440051495981e-05\n",
      "Epoch: 26 Loss: 2.6710710367559657e-05\n",
      "Epoch: 27 Loss: 1.5484999052947185e-05\n",
      "Epoch: 28 Loss: 8.97711788156406e-06\n",
      "Epoch: 29 Loss: 5.204304190399404e-06\n",
      "Epoch: 30 Loss: 3.0170910601247117e-06\n",
      "Epoch: 31 Loss: 1.7490980796050634e-06\n",
      "Epoch: 32 Loss: 1.0140045597235778e-06\n",
      "Epoch: 33 Loss: 5.87848822849018e-07\n",
      "Epoch: 34 Loss: 3.407935745545842e-07\n",
      "Epoch: 35 Loss: 1.9756824534648574e-07\n",
      "Epoch: 36 Loss: 1.1453623097121573e-07\n",
      "Epoch: 37 Loss: 6.640008459935845e-08\n",
      "Epoch: 38 Loss: 3.849411838860041e-08\n",
      "Epoch: 39 Loss: 2.2316193713705072e-08\n",
      "Epoch: 40 Loss: 1.2937366088966934e-08\n",
      "Epoch: 41 Loss: 7.500178725305108e-09\n",
      "Epoch: 42 Loss: 4.348078312478475e-09\n",
      "Epoch: 43 Loss: 2.520711266191204e-09\n",
      "Epoch: 44 Loss: 1.461331841505922e-09\n",
      "Epoch: 45 Loss: 8.471778500067508e-10\n",
      "Epoch: 46 Loss: 4.91134381132344e-10\n",
      "Epoch: 47 Loss: 2.847253150932652e-10\n",
      "Epoch: 48 Loss: 1.6506379551816591e-10\n",
      "Epoch: 49 Loss: 9.569242757878982e-11\n",
      "Epoch: 50 Loss: 5.5475767213115696e-11\n",
      "Epoch: 51 Loss: 3.216096432026092e-11\n",
      "Epoch: 52 Loss: 1.864467454756676e-11\n",
      "Epoch: 53 Loss: 1.0808876430600137e-11\n",
      "Epoch: 54 Loss: 6.266229507900754e-12\n",
      "Epoch: 55 Loss: 3.6327209758001188e-12\n",
      "Epoch: 56 Loss: 2.1059971860211514e-12\n",
      "Epoch: 57 Loss: 1.2209096641094938e-12\n",
      "Epoch: 58 Loss: 7.077979122146375e-13\n",
      "Epoch: 59 Loss: 4.103316541096371e-13\n",
      "Epoch: 60 Loss: 2.3788155395030217e-13\n",
      "Epoch: 61 Loss: 1.3790706325249438e-13\n",
      "Epoch: 62 Loss: 7.994885651050372e-14\n",
      "Epoch: 63 Loss: 4.634874697194613e-14\n",
      "Epoch: 64 Loss: 2.6869757160950258e-14\n",
      "Epoch: 65 Loss: 1.5577203458427198e-14\n",
      "Epoch: 66 Loss: 9.030571569297535e-15\n",
      "Epoch: 67 Loss: 5.235292840303989e-15\n",
      "Epoch: 68 Loss: 3.0350561761484055e-15\n",
      "Epoch: 69 Loss: 1.7595130764716369e-15\n",
      "Epoch: 70 Loss: 1.0200424123213805e-15\n",
      "Epoch: 71 Loss: 5.913491438782364e-16\n",
      "Epoch: 72 Loss: 3.428228283668558e-16\n",
      "Epoch: 73 Loss: 1.987446665595714e-16\n",
      "Epoch: 74 Loss: 1.1521824880640833e-16\n",
      "Epoch: 75 Loss: 6.679546182588113e-17\n",
      "Epoch: 76 Loss: 3.8723339985200335e-17\n",
      "Epoch: 77 Loss: 2.244907526892778e-17\n",
      "Epoch: 78 Loss: 1.3014396438733774e-17\n",
      "Epoch: 79 Loss: 7.544842593015808e-18\n",
      "Epoch: 80 Loss: 4.373968578189236e-18\n",
      "Epoch: 81 Loss: 2.5357179263728307e-18\n",
      "Epoch: 82 Loss: 1.4700342658091236e-18\n",
      "Epoch: 83 Loss: 8.522231425840035e-19\n",
      "Epoch: 84 Loss: 4.940583656539439e-19\n",
      "Epoch: 85 Loss: 2.864203778064953e-19\n",
      "Epoch: 86 Loss: 1.6604668640010368e-19\n",
      "Epoch: 87 Loss: 9.626253239972439e-20\n",
      "Epoch: 88 Loss: 5.580615085281654e-20\n",
      "Epoch: 89 Loss: 3.2352380980702597e-20\n",
      "Epoch: 90 Loss: 1.875579093459931e-20\n",
      "Epoch: 91 Loss: 1.0873050982280488e-20\n",
      "Epoch: 92 Loss: 6.303449940039761e-21\n",
      "Epoch: 93 Loss: 3.654347533276102e-21\n",
      "Epoch: 94 Loss: 2.1185014889090856e-21\n",
      "Epoch: 95 Loss: 1.2282664329742545e-21\n",
      "Epoch: 96 Loss: 7.120115344381946e-22\n",
      "Epoch: 97 Loss: 4.127837946085385e-22\n",
      "Epoch: 98 Loss: 2.393024457570439e-22\n",
      "Epoch: 99 Loss: 1.3872444107890534e-22\n",
      "Epoch: 100 Loss: 8.043983411646811e-23\n",
      "Epoch: 101 Loss: 4.66258951712393e-23\n",
      "Epoch: 102 Loss: 2.702448844644748e-23\n",
      "Epoch: 103 Loss: 1.567060276421871e-23\n",
      "Epoch: 104 Loss: 9.081720939450732e-24\n",
      "Epoch: 105 Loss: 5.267268440371388e-24\n",
      "Epoch: 106 Loss: 3.049070460980425e-24\n",
      "Epoch: 107 Loss: 1.7702070267595714e-24\n",
      "Epoch: 108 Loss: 1.025203632425227e-24\n",
      "Epoch: 109 Loss: 5.943467386552356e-25\n",
      "Epoch: 110 Loss: 3.446698955700697e-25\n",
      "Epoch: 111 Loss: 1.9958890876906298e-25\n",
      "Epoch: 112 Loss: 1.15717217325965e-25\n",
      "Epoch: 113 Loss: 6.68015303150205e-26\n",
      "Epoch: 114 Loss: 3.8878220852912346e-26\n",
      "Epoch: 115 Loss: 2.279808016088724e-26\n",
      "Epoch: 116 Loss: 1.2924697071141057e-26\n",
      "Epoch: 117 Loss: 7.576220133742597e-27\n",
      "Epoch: 118 Loss: 4.319802316990261e-27\n",
      "Epoch: 119 Loss: 2.473867798773093e-27\n",
      "Epoch: 120 Loss: 1.4586038137536508e-27\n",
      "Epoch: 121 Loss: 8.590695257856819e-28\n",
      "Epoch: 122 Loss: 4.930380657631324e-28\n",
      "Epoch: 123 Loss: 2.8477878678478526e-28\n",
      "Epoch: 124 Loss: 1.7749370367472766e-28\n",
      "Epoch: 125 Loss: 7.888609052210118e-29\n",
      "Epoch: 126 Loss: 5.048709793414476e-29\n",
      "Epoch: 127 Loss: 2.8398992587956425e-29\n",
      "Epoch: 128 Loss: 1.9721522630525295e-29\n",
      "Epoch: 129 Loss: 1.262177448353619e-29\n",
      "Epoch: 130 Loss: 1.262177448353619e-29\n",
      "Epoch: 131 Loss: 7.099748146989106e-30\n",
      "Epoch: 132 Loss: 3.1554436208840472e-30\n",
      "Epoch: 133 Loss: 3.1554436208840472e-30\n",
      "Epoch: 134 Loss: 3.1554436208840472e-30\n",
      "Epoch: 135 Loss: 7.888609052210118e-31\n",
      "Epoch: 136 Loss: 7.888609052210118e-31\n",
      "Epoch: 137 Loss: 7.888609052210118e-31\n",
      "Epoch: 138 Loss: 7.888609052210118e-31\n",
      "Epoch: 139 Loss: 7.888609052210118e-31\n",
      "Epoch: 140 Loss: 7.888609052210118e-31\n",
      "Epoch: 141 Loss: 7.888609052210118e-31\n",
      "Epoch: 142 Loss: 7.888609052210118e-31\n",
      "Epoch: 143 Loss: 7.888609052210118e-31\n",
      "Epoch: 144 Loss: 7.888609052210118e-31\n",
      "Epoch: 145 Loss: 7.888609052210118e-31\n",
      "Epoch: 146 Loss: 7.888609052210118e-31\n",
      "Epoch: 147 Loss: 7.888609052210118e-31\n",
      "Epoch: 148 Loss: 7.888609052210118e-31\n",
      "Epoch: 149 Loss: 7.888609052210118e-31\n",
      "Epoch: 150 Loss: 7.888609052210118e-31\n",
      "Epoch: 151 Loss: 7.888609052210118e-31\n",
      "Epoch: 152 Loss: 7.888609052210118e-31\n",
      "Epoch: 153 Loss: 7.888609052210118e-31\n",
      "Epoch: 154 Loss: 7.888609052210118e-31\n",
      "Epoch: 155 Loss: 7.888609052210118e-31\n",
      "Epoch: 156 Loss: 7.888609052210118e-31\n",
      "Epoch: 157 Loss: 7.888609052210118e-31\n",
      "Epoch: 158 Loss: 7.888609052210118e-31\n",
      "Epoch: 159 Loss: 7.888609052210118e-31\n",
      "Epoch: 160 Loss: 7.888609052210118e-31\n",
      "Epoch: 161 Loss: 7.888609052210118e-31\n",
      "Epoch: 162 Loss: 7.888609052210118e-31\n",
      "Epoch: 163 Loss: 7.888609052210118e-31\n",
      "Epoch: 164 Loss: 7.888609052210118e-31\n",
      "Epoch: 165 Loss: 7.888609052210118e-31\n",
      "Epoch: 166 Loss: 7.888609052210118e-31\n",
      "Epoch: 167 Loss: 7.888609052210118e-31\n",
      "Epoch: 168 Loss: 7.888609052210118e-31\n",
      "Epoch: 169 Loss: 7.888609052210118e-31\n",
      "Epoch: 170 Loss: 7.888609052210118e-31\n",
      "Epoch: 171 Loss: 7.888609052210118e-31\n",
      "Epoch: 172 Loss: 7.888609052210118e-31\n",
      "Epoch: 173 Loss: 7.888609052210118e-31\n",
      "Epoch: 174 Loss: 7.888609052210118e-31\n",
      "Epoch: 175 Loss: 7.888609052210118e-31\n",
      "Epoch: 176 Loss: 7.888609052210118e-31\n",
      "Epoch: 177 Loss: 7.888609052210118e-31\n",
      "Epoch: 178 Loss: 7.888609052210118e-31\n",
      "Epoch: 179 Loss: 7.888609052210118e-31\n",
      "Epoch: 180 Loss: 7.888609052210118e-31\n",
      "Epoch: 181 Loss: 7.888609052210118e-31\n",
      "Epoch: 182 Loss: 7.888609052210118e-31\n",
      "Epoch: 183 Loss: 7.888609052210118e-31\n",
      "Epoch: 184 Loss: 7.888609052210118e-31\n",
      "Epoch: 185 Loss: 7.888609052210118e-31\n",
      "Epoch: 186 Loss: 7.888609052210118e-31\n",
      "Epoch: 187 Loss: 7.888609052210118e-31\n",
      "Epoch: 188 Loss: 7.888609052210118e-31\n",
      "Epoch: 189 Loss: 7.888609052210118e-31\n",
      "Epoch: 190 Loss: 7.888609052210118e-31\n",
      "Epoch: 191 Loss: 7.888609052210118e-31\n",
      "Epoch: 192 Loss: 7.888609052210118e-31\n",
      "Epoch: 193 Loss: 7.888609052210118e-31\n",
      "Epoch: 194 Loss: 7.888609052210118e-31\n",
      "Epoch: 195 Loss: 7.888609052210118e-31\n",
      "Epoch: 196 Loss: 0.0\n",
      "Epoch: 197 Loss: 0.0\n",
      "Epoch: 198 Loss: 0.0\n",
      "Epoch: 199 Loss: 0.0\n",
      "Epoch: 200 Loss: 0.0\n",
      "Optimized weights: [ 0.32623638 -1.22363789  1.21559095]\n",
      "Optimized bias: 0.10519698239731712\n"
     ]
    }
   ],
   "source": [
    "optimized_weights = []\n",
    "optimized_bias = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    # forward pass\n",
    "    linear_output = np.dot(inputs, weights) + bias\n",
    "    relu_output = activation_relu(linear_output)\n",
    "\n",
    "    loss = (relu_output - true_value) ** 2\n",
    "\n",
    "    # backpropagation\n",
    "    dloss_drelu = 2 * (relu_output - true_value)\n",
    "    drelu_dsum = derivative_relu(relu_output)\n",
    "    dmul_dweights = inputs\n",
    "    dbias_dbias = 1\n",
    "\n",
    "    dloss_dweights = dloss_drelu * drelu_dsum * dmul_dweights\n",
    "    dloss_dbias = dloss_drelu * drelu_dsum * dbias_dbias\n",
    "    \n",
    "    # gradient decent\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    bias -= learning_rate * dloss_dbias\n",
    "\n",
    "    optimized_weights.append(weights.copy())\n",
    "    optimized_bias.append(bias)\n",
    "\n",
    "    print(\"Epoch:\", epoch+1, \"Loss:\", loss)\n",
    "\n",
    "print(\"Optimized weights:\", optimized_weights[len(optimized_weights)-2])\n",
    "print(\"Optimized bias:\", optimized_bias[len(optimized_bias)-2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_from_scratch",
   "language": "python",
   "name": "nn_from_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
