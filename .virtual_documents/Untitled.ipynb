import numpy as np





class Loss_CategoricalCrossEntropy:
    def forward(self, y_pred, y_true):
        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)
        y_true = y_true

        if len(y_pred.shape) == 1:
            correct_confidences = y_pred_clipped[range(len(y_pred_clipped)), y_true]

        if len(y_pred.shape) == 2:
            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)

        negative_log_likelihoods = -np.log(correct_confidences)
        return negative_log_likelihoods

    def backward(self, y_pred,y_true):
        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)
        no_of_batches = len(y_pred)
        no_of_labels = len(y_pred[0])
        
        if len(y_true.shape) == 1:
            y_true = np.eye(no_of_labels)[y_true]

        # Calculating gradient
        self.dL_dycap = -y_true / y_pred # Here ycap means the predictions
        
        # Applying the normalization on gradient
        self.normalized_dL_dycap = self.dL_dycap / no_of_batches

y_pred = np.array([[.1, .5, .3, .1],
                   [.2, .1, .4, .3],
                   [.3, .2, .1, .4]])
y_true = np.array([1, 2, 3])
loss_function = Loss_CategoricalCrossEntropy()
loss_function.backward(y_pred, y_true)
print(loss_function.dL_dycap)
print(loss_function.normalized_dL_dycap)
