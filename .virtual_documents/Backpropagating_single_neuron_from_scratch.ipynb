import numpy as np





inputs = np.array([1.0, -2.0, 3.0])
weights = np.array([-3.0, -1.0, 2.0])
bias = 1.0
target_output = 0.0
learning_rate = 0.001





def activation_relu(x):
    return np.maximum(0, x)

def derivative_relu(x):
    return np.where(x > 0, 1.0, 0.0)





for epoch in range(200):
    # neuron output
    neuron_output = np.dot(inputs, weights) + bias
    relu_output = activation_relu(neuron_output)
    
    # square loss
    loss = (relu_output - target_output)**2

    # Backward pass (Backpropagation)
    dloss_drelu = 2 * relu_output
    drelu_dsum = derivative_relu(neuron_output)
    dmul_dweights = inputs
    dbias = 1.0

    dloss_dweights = dloss_drelu * drelu_dsum * dmul_dweights
    dloss_dbias = dloss_drelu * drelu_dsum * dbias

    weights -= learning_rate * dloss_dweights
    bias -= learning_rate * dloss_dbias

    print("Epoch:", epoch, "loss:", loss)

print("Final weights:", weights)
print("Final biases:", bias)
