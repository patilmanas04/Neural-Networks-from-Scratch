import numpy as np








inputs = np.array([[1, 2, 3, 2.5],
                   [2, 5, -1, 2],
                   [-1.5, 2.7, 3.3, -0.8]])

dL_dz = np.array([[1, 1, 1],
                  [2, 2, 2],
                  [3, 3, 3]])

# Gradient of loss with respect to weights
dL_dw = np.dot(inputs.T, dL_dz)
print(dL_dw)





# Gradient of loss with respect to 
dL_db = np.sum(dL_dz, axis=0, keepdims=True)
print(dL_db)





weights = np.array([[0.2, 0.8, -0.5, 1],
                    [0.5, -0.91, 0.26, -0.5],
                    [-0.26, -0.27, 0.17, 0.87]]).T
dL_dX = np.dot(dL_dz, weights.T)
print(dL_dX)





class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
        self.n_inputs = n_inputs
        self.n_neurons = n_neurons
        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons))

    def forward(self, inputs):
        self.inputs = inputs
        self.output = np.dot(inputs, self.weights) + self.biases

    def backward(self, dL_dz):
        self.dL_dw = np.dot(self.inputs.T, dL_dz)
        self.dL_db = np.sum(dL_dz, axis=0, keepdims=True)
        self.dL_dX = np.dot(dL_dz, self.weights.T)





class Activation_ReLU:
    def forward(self, inputs):
        self.inputs = inputs
        self.output = np.maximum(0, inputs)

    def backward(self, dL_da):
        # Here dL_da is the gradient of L with respect to a. a is the output of the ReLU activation function, a = ReLU(z)
        # We have been given dL_da and we have to find dL_dz (gradient of loss with respect to z), here z is the output of the previous layer which will pass through the activation function
        # So we are basically backpropagating through the relu activation function
        self.dL_dz = dL_da
        self.dL_dz[self.inputs<=0] = 0





class Loss_CategoricalCrossEntropy:
    def calculate(self, y_pred, y_true):
        negative_log_likelihoods = self.forward(y_pred, y_true)
        avg_loss = np.mean(negative_log_likelihoods)
        return avg_loss
    
    def forward(self, y_pred, y_true):
        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)
        y_true = y_true

        if len(y_true.shape) == 1:
            correct_confidences = y_pred_clipped[range(len(y_pred_clipped)), y_true]

        if len(y_true.shape) == 2:
            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)

        negative_log_likelihoods = -np.log(correct_confidences)
        return negative_log_likelihoods

    def backward(self, y_pred,y_true):
        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)
        no_of_batches = len(y_pred)
        no_of_labels = len(y_pred[0])
        
        if len(y_true.shape) == 1:
            y_true = np.eye(no_of_labels)[y_true]

        # Calculating gradient
        self.dL_dycap = -y_true / y_pred # Here ycap means the predictions
        
        # Applying the normalization on gradient
        self.normalized_dL_dycap = self.dL_dycap / no_of_batches

y_pred = np.array([[.1, .5, .3, .1],
                   [.2, .1, .4, .3],
                   [.3, .2, .1, .4]])
y_true = np.array([1, 2, 3])
loss_function = Loss_CategoricalCrossEntropy()
loss_function.backward(y_pred, y_true)
print(loss_function.dL_dycap)
print(loss_function.normalized_dL_dycap)





class Activation_Softmax:
    def forward(self, final_layer_outputs):
        exp_values = np.exp(final_layer_outputs - np.max(final_layer_outputs, axis=1, keepdims=True))
        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)
        self.output = probabilities

class Activation_Softmax_Loss_CategoricalCrossEntropy:
    def __init__(self):
        self.activation_softmax = Activation_Softmax()
        self.loss_function = Loss_CategoricalCrossEntropy()
    
    def forward(self, final_layer_outputs, y_true):
        # Calculating softmax activation outputs
        self.activation_softmax.forward(final_layer_outputs)
        self.output = self.activation_softmax.output

        # Calculating loss
        self.loss = self.loss_function.calculate(self.output, y_true)
        return self.loss

    def backward(self, softmax_outputs, y_true):
        no_of_batches = len(softmax_outputs)

        # Converting the y_true to discrete vector if it is in one hot encoding
        if len(y_true.shape) == 2:
            y_true = np.argmax(y_true, axis=1)

        self.dL_dz = softmax_outputs.copy()
        # Calculating gradient
        self.dL_dz[range(no_of_batches), y_true] -= 1

        # Applying normalization
        self.dL_dz = self.dL_dz / no_of_batches

softmax_outputs = np.array([[.7, .1, .2],
                            [.1, .5, .4],
                            [.02, .9, .08]])
class_targets = np.array([0, 1, 1])
softmax_loss = Activation_Softmax_Loss_CategoricalCrossEntropy()
softmax_loss.backward(softmax_outputs, class_targets)
normalized_dL_dz = softmax_loss.dL_dz
print("Gradients: Combined loss and softmax activation ->")
print(normalized_dL_dz)








from nnfs.datasets import spiral_data
import matplotlib.pyplot as plt

X, y = spiral_data(samples=100, classes=3)
print(X[:5])
plt.scatter(X[:, 0], X[:, 1], c=y, cmap="brg")
plt.show()





# Defining learning rate
learning_rate = 0.001

# Defining the classes
dense_layer1 = Layer_Dense(2, 3)
activation_relu = Activation_ReLU()
dense_layer2 = Layer_Dense(3, 3)
loss_activation_softmax = Activation_Softmax_Loss_CategoricalCrossEntropy()

# Forward pass
dense_layer1.forward(X)
activation_relu.forward(dense_layer1.output)
dense_layer2.forward(activation_relu.output)
loss_activation_softmax.forward(dense_layer2.output, y)
print("Softmax Outputs ->\n", loss_activation_softmax.output[:3])

# Printing performance matrices
print("Loss:", loss_activation_softmax.loss)
predictions = np.argmax(loss_activation_softmax.output, axis=1)
accuracy = np.mean(predictions==y)
print("Accuracy:", accuracy)

# Backward pass
loss_activation_softmax.backward(loss_activation_softmax.output, y)
dense_layer2.backward(loss_activation_softmax.dL_dz)
activation_relu.backward(dense_layer2.dL_dX)
dense_layer1.backward(activation_relu.dL_dz)

# Gradients of weights and biases
print(dense_layer1.dL_dw)
print(dense_layer1.dL_db)
print(dense_layer2.dL_dw)
print(dense_layer2.dL_db)





dense_layer1.weights -= learning_rate*dense_layer1.dL_dw
dense_layer1.biases -= learning_rate*dense_layer1.dL_db
dense_layer2.weights -= learning_rate*dense_layer2.dL_dw
dense_layer2.biases -= learning_rate*dense_layer2.dL_db





# Forward pass
dense_layer1.forward(X)
activation_relu.forward(dense_layer1.output)
dense_layer2.forward(activation_relu.output)
loss_activation_softmax.forward(dense_layer2.output, y)
print("Softmax Outputs ->\n", loss_activation_softmax.output[:3])

# Printing performance matrices
print("Loss:", loss_activation_softmax.loss)
predictions = np.argmax(loss_activation_softmax.output, axis=1)
accuracy = np.mean(predictions==y)
print("Accuracy:", accuracy)
